@String{ACL01 = "Proceedings of the 39th Annual Conference of the Association for Computational Linguistics (ACL-01)"}

@article{akmajian,
author = {Akmajian, Adrian and Ray Jackendoff},
year   = {1970},
title  = {Coreferentiality and Stress},
journal = {Linguistic Inquiry}, 
volume = {1},
number = {1},
pages = {124--126},
}

@article{woods,
author = {Woods, William A.}, 
year = {1970},
title = {Transition Network Grammars for Natural
Language Analysis},
journal = {Communications of the {ACM}}, 
volume = {13},
number = {10},
pages = {591--606},
}

@book{altenberg,
author = {Altenberg, Bengt},
year =  {1987},
title = {Prosodic Patterns in Spoken English: Studies
in the Correlation between Prosody and Grammar for Text-to-Speech
Conversion}, 
volume = {76},
series = {Lund Studies in English},
publisher = {Lund University Press}, 
address = {Lund},
}

@book{winograd,
author = {Winograd, Terry},
year =  {1972},
title =  {Understanding Natural Language},
publisher =  {Academic Press},
address = {New York},
}

@incollection{cutler,
author = {Cutler, Anne},
year = {1983},
title = {Speakers' Conception of the Functions of Prosody},
editor =  {Anne Cutler and D. Robert Ladd},
booktitle = {Prosody: Models and Measurements},
publisher = {Springer-Verlag}, 
address = {Berlin}, 
pages = {79--92}, 
}

@incollection{sgall, 
author = {Sgall, Petr},
year = {1970},
title =  {L'ordre des mots et la semantique},
editor = {Ferenc Kiefer},
booktitle = {Studies in Syntax and Semantics}, 
publisher = {D. Reidel}, 
address = {New York},
pages  = {231--240},
}

@inbook{jurafsky,
author = {Jurafsky, Daniel and James H. Martin},
year =  {2000},
title = {Speech and Language Processing}, 
chapter = {1},
publisher = {Prentice Hall},
}

@techreport{appelt,
author = {Appelt, Douglas E.},
year = {1982}, 
title = {Planning Natural-Language Utterances to
Satisfy Multiple Goals},
number = {259}, 
institution = {SRI},
}

@techreport{robinson,
author = {Robinson, Jane J.},
year =  {1964},
title = {Automatic Parsing and Fact Retrieval: A
Comment on Grammar, Paraphrase, and Meaning},
type = {Memorandum}, 
number = {RM-3892-PR},
institution = {The RAND Corporation}, 
address = {Santa Monica, CA},
}

@phdthesis{baart,
author = {Baart, J. L. G.},
year = {1987},
title = {Focus, Syntax, and Accent Placement},
school = {University of Leyden}, 
address = {Leyden},
}

@phdthesis{spaerckjones,
author = {Sp\"arck Jones, Karen}, 
year = {1964},
title  =  {Synonymy and Semantic Classification},
type =  {{D.Phil.}\ dissertation}, 
school = {Cambridge University}, 
address = {Cambridge, England},
}

@mastersthesis{cahn,
author = {Cahn, Janet E.},
year =  {1989},
title =  {Generating Expression in Synthesized Speech},
school = {Massachusetts Institute of Technology}, 
month = {May},
}

@unpublished{ayers, 
author = {Ayers, Gail M.},
year = {1992},  
title = {Discourse Functions of Pitch Range in Spontaneous
and Read Speech},
note =  {Paper presented at the Linguistic Society of America
annual meeting}, 
}

@proceedings{benoit, 
editor = {Benoit, Christian and Gerard Bailly}, 
year = {1989},
title = {Proceedings of the
Eurpoean Speech Communication Association Workshop on Speech
Synthesis, \emph{Autrans, September}},
organization = {European Speech Communication Association},
publisher = {Institut de la Communication Parlee},
address = {Grenoble},
}

@inproceedings{krahmer, 
author = {Krahmer, Emiel and  M. Swerts and Mariet Theune and M. Weegels},
year = {1999},
title =  {Error Spotting in Human-Machine Interactions},
booktitle =  {Proceedings of EUROSPEECH-99}, 
pages = {1423--1426}, 
address = {Budapest},
}

@inproceedings{Copestake2001,
	Author = {Copestake, Ann and Lascarides, Alex and Flickinger, Dan},
	Booktitle = ACL01,
	Date-Added = {2014-07-07 11:55:56 +0000},
	Date-Modified = {2014-07-07 11:55:56 +0000},
	Title = {{An Algebra for Semantic Construction in Constraint-Based Grammars}},
        pages     = {140--147},
	address="Toulouse, France",
	Year = {2001}}

@Book{vneumann,
  author =       "John von Neumann",
  title =        "Collected Works: Volume {V}",
  publisher =    "Macmillan Company",
  address =      "New York",
  year =         "1963",
}

%%% Bib Hieronder zetten


@article{belinkov_analysis_2019,
	title = {Analysis {Methods} in {Neural} {Language} {Processing}: {A} {Survey}},
	shorttitle = {Analysis {Methods} in {Neural} {Language} {Processing}},
	url = {http://arxiv.org/abs/1812.08951},
	abstract = {The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.},
	urldate = {2020-01-27},
	journal = {arXiv:1812.08951 [cs]},
	author = {Belinkov, Yonatan and Glass, James},
	month = jan,
	year = {2019},
	note = {arXiv: 1812.08951},
	keywords = {68T50, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, I.2.7, Recommended},
	file = {arXiv Fulltext PDF:/home/jokmenen/Zotero/storage/PX2MWDJV/Belinkov and Glass - 2019 - Analysis Methods in Neural Language Processing A .pdf:application/pdf;arXiv.org Snapshot:/home/jokmenen/Zotero/storage/ZH9Z28KJ/1812.html:text/html}
}

@article{alishahi_analyzing_2019,
	title = {Analyzing and interpreting neural networks for {NLP}: {A} report on the first {BlackboxNLP} workshop},
	volume = {25},
	issn = {1351-3249, 1469-8110},
	shorttitle = {Analyzing and interpreting neural networks for {NLP}},
	url = {https://www.cambridge.org/core/product/identifier/S135132491900024X/type/journal_article},
	doi = {10.1017/S135132491900024X},
	abstract = {Abstract
            The Empirical Methods in Natural Language Processing (EMNLP) 2018 workshop BlackboxNLP was dedicated to resources and techniques specifically developed for analyzing and understanding the inner-workings and representations acquired by neural models of language. Approaches included: systematic manipulation of input to neural networks and investigating the impact on their performance, testing whether interpretable knowledge can be decoded from intermediate representations acquired by neural networks, proposing modifications to neural network architectures to make their knowledge state or generated output more explainable, and examining the performance of networks on simplified or formal languages. Here we review a number of representative studies in each category.},
	language = {en},
	number = {4},
	urldate = {2020-01-27},
	journal = {Natural Language Engineering},
	author = {Alishahi, Afra and Chrupała, Grzegorz and Linzen, Tal},
	month = jul,
	year = {2019},
	keywords = {Recommended},
	pages = {543--557},
	file = {Alishahi et al. - 2019 - Analyzing and interpreting neural networks for NLP.pdf:/home/jokmenen/Zotero/storage/Y8VQUZUK/Alishahi et al. - 2019 - Analyzing and interpreting neural networks for NLP.pdf:application/pdf}
}

@misc{noauthor_correlating_nodate,
	title = {Correlating neural and symbolic representations of language {\textbar} {OpenReview}},
	url = {https://openreview.net/forum?id=ryx35Ehi84},
	urldate = {2020-01-27},
	keywords = {Recommended},
	file = {Correlating neural and symbolic representations of language | OpenReview:/home/jokmenen/Zotero/storage/EKUL9T64/forum.html:text/html}
}

@article{zeiler_visualizing_2013,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classiﬁcation performance on the ImageNet benchmark (Krizhevsky et al., 2012). However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classiﬁer. Used in a diagnostic role, these visualizations allow us to ﬁnd model architectures that outperform Krizhevsky et al. on the ImageNet classiﬁcation benchmark. We also perform an ablation study to discover the performance contribution from diﬀerent model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classiﬁer is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	language = {en},
	urldate = {2020-01-27},
	journal = {arXiv:1311.2901 [cs]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = nov,
	year = {2013},
	note = {arXiv: 1311.2901},
	keywords = {Recommended, Computer Science - Computer Vision and Pattern Recognition},
	file = {Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:/home/jokmenen/Zotero/storage/4XQTWJ5F/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf}
}

@article{kadar_representation_2017,
	title = {Representation of {Linguistic} {Form} and {Function} in {Recurrent} {Neural} {Networks}},
	volume = {43},
	issn = {0891-2017},
	url = {https://doi.org/10.1162/COLI_a_00300},
	doi = {10.1162/COLI_a_00300},
	abstract = {We present novel methods for analyzing the activation patterns of recurrent neural networks from a linguistic point of view and explore the types of linguistic structure they learn. As a case study, we use a standard standalone language model, and a multi-task gated recurrent network architecture consisting of two parallel pathways with shared word embeddings: The Visual pathway is trained on predicting the representations of the visual scene corresponding to an input sentence, and the Textual pathway is trained to predict the next word in the same sentence. We propose a method for estimating the amount of contribution of individual tokens in the input to the final prediction of the networks. Using this method, we show that the Visual pathway pays selective attention to lexical categories and grammatical functions that carry semantic information, and learns to treat word types differently depending on their grammatical function and their position in the sequential structure of the sentence. In contrast, the language models are comparatively more sensitive to words with a syntactic function. Further analysis of the most informative n-gram contexts for each model shows that in comparison with the Visual pathway, the language models react more strongly to abstract contexts that represent syntactic constructions.},
	number = {4},
	urldate = {2020-01-27},
	journal = {Computational Linguistics},
	author = {Kádár, Ákos and Chrupała, Grzegorz and Alishahi, Afra},
	month = sep,
	year = {2017},
	keywords = {Recommended},
	pages = {761--780},
	file = {Snapshot:/home/jokmenen/Zotero/storage/YJSZVB5N/COLI_a_00300.html:text/html;Full Text:/home/jokmenen/Zotero/storage/LVCKAMSE/Kádár et al. - 2017 - Representation of Linguistic Form and Function in .pdf:application/pdf}
}

@article{alishahi_encoding_2017,
	title = {Encoding of phonology in a recurrent neural model of grounded speech},
	url = {http://arxiv.org/abs/1706.03815},
	doi = {10.18653/v1/K17-1037},
	abstract = {We study the representation and encoding of phonemes in a recurrent neural network model of grounded speech. We use a model which processes images and their spoken descriptions, and projects the visual and auditory representations into the same semantic space. We perform a number of analyses on how information about individual phonemes is encoded in the MFCC features extracted from the speech signal, and the activations of the layers of the model. Via experiments with phoneme decoding and phoneme discrimination we show that phoneme representations are most salient in the lower layers of the model, where low-level signals are processed at a fine-grained level, although a large amount of phonological information is retain at the top recurrent layer. We further find out that the attention mechanism following the top recurrent layer significantly attenuates encoding of phonology and makes the utterance embeddings much more invariant to synonymy. Moreover, a hierarchical clustering of phoneme representations learned by the network shows an organizational structure of phonemes similar to those proposed in linguistics.},
	urldate = {2020-01-27},
	journal = {Proceedings of the 21st Conference on Computational Natural Language           Learning (CoNLL 2017)},
	author = {Alishahi, Afra and Barking, Marie and Chrupała, Grzegorz},
	year = {2017},
	note = {arXiv: 1706.03815},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Recommended, Computer Science - Sound},
	pages = {368--378},
	file = {arXiv Fulltext PDF:/home/jokmenen/Zotero/storage/EE7X3RRJ/Alishahi et al. - 2017 - Encoding of phonology in a recurrent neural model .pdf:application/pdf;arXiv.org Snapshot:/home/jokmenen/Zotero/storage/BGXCI8TW/1706.html:text/html}
}

@article{chrupala_representations_2017,
	title = {Representations of language in a model of visually grounded speech signal},
	url = {http://arxiv.org/abs/1702.01991},
	doi = {10.18653/v1/P17-1057},
	abstract = {We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.},
	urldate = {2020-01-27},
	journal = {Proceedings of the 55th Annual Meeting of the Association for           Computational Linguistics (Volume 1: Long Papers)},
	author = {Chrupała, Grzegorz and Gelderloos, Lieke and Alishahi, Afra},
	year = {2017},
	note = {arXiv: 1702.01991},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Recommended, Computer Science - Artificial Intelligence},
	pages = {613--622},
	file = {arXiv Fulltext PDF:/home/jokmenen/Zotero/storage/VYSJLTBQ/Chrupała et al. - 2017 - Representations of language in a model of visually.pdf:application/pdf;arXiv.org Snapshot:/home/jokmenen/Zotero/storage/6YGZZC7I/1702.html:text/html}
}

@article{wu_faithful_2019,
	title = {Faithful {Multimodal} {Explanation} for {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/1809.02805},
	abstract = {AI systems' ability to explain their reasoning is critical to their utility and trustworthiness. Deep neural networks have enabled significant progress on many challenging problems such as visual question answering (VQA). However, most of them are opaque black boxes with limited explanatory capability. This paper presents a novel approach to developing a high-performing VQA system that can elucidate its answers with integrated textual and visual explanations that faithfully reflect important aspects of its underlying reasoning while capturing the style of comprehensible human explanations. Extensive experimental evaluation demonstrates the advantages of this approach compared to competing methods with both automatic evaluation metrics and human evaluation metrics.},
	urldate = {2020-02-02},
	journal = {arXiv:1809.02805 [cs]},
	author = {Wu, Jialin and Mooney, Raymond J.},
	month = jun,
	year = {2019},
	note = {arXiv: 1809.02805},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Explanations},
	file = {arXiv.org Snapshot:/home/jokmenen/Zotero/storage/AQP5VI6Q/1809.html:text/html;arXiv Fulltext PDF:/home/jokmenen/Zotero/storage/VGXRNCHY/Wu en Mooney - 2019 - Faithful Multimodal Explanation for Visual Questio.pdf:application/pdf}
}

@article{montavon_methods_2018,
	title = {Methods for interpreting and understanding deep neural networks},
	volume = {73},
	issn = {1051-2004},
	url = {http://www.sciencedirect.com/science/article/pii/S1051200417302385},
	doi = {10.1016/j.dsp.2017.10.011},
	abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.},
	language = {en},
	urldate = {2020-01-29},
	journal = {Digital Signal Processing},
	author = {Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
	month = feb,
	year = {2018},
	keywords = {Activation maximization, Deep neural networks, Layer-wise relevance propagation, Nog lezen, Sensitivity analysis, Taylor decomposition},
	pages = {1--15},
	file = {ScienceDirect Full Text PDF:/home/jokmenen/Zotero/storage/LESA457A/Montavon et al. - 2018 - Methods for interpreting and understanding deep ne.pdf:application/pdf;ScienceDirect Snapshot:/home/jokmenen/Zotero/storage/WL5CHIBP/S1051200417302385.html:text/html}
}

@article{goyal_making_2017,
	title = {Making the {V} in {VQA} {Matter}: {Elevating} the {Role} of {Image} {Understanding} in {Visual} {Question} {Answering}},
	shorttitle = {Making the {V} in {VQA} {Matter}},
	url = {http://arxiv.org/abs/1612.00837},
	abstract = {Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability. We propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at www.visualqa.org as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0). We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners. Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.},
	urldate = {2020-02-03},
	journal = {arXiv:1612.00837 [cs]},
	author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
	month = may,
	year = {2017},
	note = {arXiv: 1612.00837},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/jokmenen/Zotero/storage/CTXT3FDI/Goyal et al. - 2017 - Making the V in VQA Matter Elevating the Role of .pdf:application/pdf;arXiv.org Snapshot:/home/jokmenen/Zotero/storage/MA72N958/1612.html:text/html}
}

@article{agrawal_vqa_2016,
	title = {{VQA}: {Visual} {Question} {Answering}},
	shorttitle = {{VQA}},
	url = {http://arxiv.org/abs/1505.00468},
	abstract = {We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ∼0.25M images, ∼0.76M questions, and ∼10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa).},
	language = {en},
	urldate = {2020-02-03},
	journal = {arXiv:1505.00468 [cs]},
	author = {Agrawal, Aishwarya and Lu, Jiasen and Antol, Stanislaw and Mitchell, Margaret and Zitnick, C. Lawrence and Batra, Dhruv and Parikh, Devi},
	month = oct,
	year = {2016},
	note = {arXiv: 1505.00468},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Agrawal et al. - 2016 - VQA Visual Question Answering.pdf:/home/jokmenen/Zotero/storage/32MP6PS3/Agrawal et al. - 2016 - VQA Visual Question Answering.pdf:application/pdf}
}

@article{lu_hierarchical_2017,
	title = {Hierarchical {Question}-{Image} {Co}-{Attention} for {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/1606.00061},
	abstract = {A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling “where to look” or visual attention, it is equally important to model “what words to listen to” or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3\% to 60.5\%, and from 61.6\% to 63.3\% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1\% for VQA and 65.4\% for COCO-QA.1.},
	language = {en},
	urldate = {2020-02-03},
	journal = {arXiv:1606.00061 [cs]},
	author = {Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
	month = jan,
	year = {2017},
	note = {arXiv: 1606.00061},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Lu et al. - 2017 - Hierarchical Question-Image Co-Attention for Visua.pdf:/home/jokmenen/Zotero/storage/9G596FQF/Lu et al. - 2017 - Hierarchical Question-Image Co-Attention for Visua.pdf:application/pdf}
}

@article{chandrasekaran_explanations_2018,
	title = {Do {Explanations} make {VQA} {Models} more {Predictable} to a {Human}?},
	url = {http://arxiv.org/abs/1810.12366},
	abstract = {A rich line of research attempts to make deep neural networks more transparent by generating human-interpretable 'explanations' of their decision process, especially for interactive tasks like Visual Question Answering (VQA). In this work, we analyze if existing explanations indeed make a VQA model -- its responses as well as failures -- more predictable to a human. Surprisingly, we find that they do not. On the other hand, we find that human-in-the-loop approaches that treat the model as a black-box do.},
	urldate = {2020-02-05},
	journal = {arXiv:1810.12366 [cs]},
	author = {Chandrasekaran, Arjun and Prabhu, Viraj and Yadav, Deshraj and Chattopadhyay, Prithvijit and Parikh, Devi},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.12366},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Explanations},
	file = {arXiv.org Snapshot:/home/jokmenen/Zotero/storage/NJ449CXZ/1810.html:text/html;arXiv Fulltext PDF:/home/jokmenen/Zotero/storage/Y2XB5XWX/Chandrasekaran e.a. - 2018 - Do Explanations make VQA Models more Predictable t.pdf:application/pdf}
}

@article{rajani_using_nodate,
	title = {Using {Explanations} to {Improve} {Ensembling} of {Visual} {Question} {Answering} {Systems}},
	abstract = {We present results on using explanations as auxiliary features to improve stacked ensembles for Visual Question Answering (VQA). VQA is a challenging task that requires systems to jointly reason about natural language and vision. We present results applying a recent ensembling approach to VQA, Stacking with Auxiliary Features (SWAF), which learns to combine the results of multiple systems. We propose using features based on explanations to improve SWAF. Using explanations we are able to improve ensembling of three recent VQA systems.},
	language = {en},
	author = {Rajani, Nazneen Fatema and Mooney, Raymond J},
	keywords = {Explanations},
	pages = {5},
	file = {Rajani and Mooney - Using Explanations to Improve Ensembling of Visual.pdf:/home/jokmenen/Zotero/storage/DSBUEWHI/Rajani and Mooney - Using Explanations to Improve Ensembling of Visual.pdf:application/pdf}
}

@misc{noauthor_regulation_2016,
	title = {Regulation ({EU}) 2016/679 of the {European} {Parliament} and of the {Council} of 27 {April} 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing {Directive} 95/46/{EC} ({General} {Data} {Protection} {Regulation}) ({Text} with {EEA} relevance)},
	url = {http://data.europa.eu/eli/reg/2016/679/oj/eng},
	language = {en},
	urldate = {2020-02-11},
	month = may,
	year = {2016}
}

@inproceedings{ribeiro_why_2016,
	address = {San Francisco, California, USA},
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://dl.acm.org/citation.cfm?doid=2939672.2939778},
	doi = {10.1145/2939672.2939778},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	language = {en},
	urldate = {2020-02-11},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} - {KDD} '16},
	publisher = {ACM Press},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year = {2016},
	keywords = {Explanations, LIME},
	pages = {1135--1144},
	file = {Ribeiro e.a. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:/home/jokmenen/Zotero/storage/63MLLEZF/Ribeiro e.a. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:application/pdf}
}


@article{tan_lxmert_2019,
	title = {{LXMERT}: {Learning} {Cross}-{Modality} {Encoder} {Representations} from {Transformers}},
	shorttitle = {{LXMERT}},
	url = {http://arxiv.org/abs/1908.07490},
	abstract = {Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22\% absolute (54\% to 76\%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results; and also present several attention visualizations for the different encoders. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert},
	language = {en},
	urldate = {2020-02-17},
	journal = {arXiv:1908.07490 [cs]},
	author = {Tan, Hao and Bansal, Mohit},
	month = dec,
	year = {2019},
	note = {arXiv: 1908.07490},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: EMNLP 2019 (14 pages; with new attention visualizations)},
	file = {Tan and Bansal - 2019 - LXMERT Learning Cross-Modality Encoder Representa.pdf:/home/jokmenen/Zotero/storage/AABUGLA7/Tan and Bansal - 2019 - LXMERT Learning Cross-Modality Encoder Representa.pdf:application/pdf}
}